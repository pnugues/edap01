{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Classifier on the *Salammb√¥* Dataset with PyTorch\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset\n",
    "We can read the data from a file with the svmlight format or directly create numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "X_train = np.array(\n",
    "    [[35680, 2217], [42514, 2761], [15162, 990], [35298, 2274],\n",
    "     [29800, 1865], [40255, 2606], [74532, 4805], [37464, 2396],\n",
    "     [31030, 1993], [24843, 1627], [36172, 2375], [39552, 2560],\n",
    "     [72545, 4597], [75352, 4871], [18031, 1119], [36961, 2503],\n",
    "     [43621, 2992], [15694, 1042], [36231, 2487], [29945, 2014],\n",
    "     [40588, 2805], [75255, 5062], [37709, 2643], [30899, 2126],\n",
    "     [25486, 1784], [37497, 2641], [40398, 2766], [74105, 5047],\n",
    "     [76725, 5312], [18317, 1215]\n",
    "     ], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.reshape((-1, 1))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaling the Data\n",
    "Scaling and normalizing are usually very significant with neural networks. We use sklean transformers. They consist of two main methods: `fit()` and `transform()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9980751 , 0.06201605],\n",
       "       [0.99789774, 0.06480679],\n",
       "       [0.9978751 , 0.06515607],\n",
       "       [0.9979313 , 0.06428964],\n",
       "       [0.99804735, 0.06246169],\n",
       "       [0.9979111 , 0.06460207],\n",
       "       [0.9979283 , 0.06433539],\n",
       "       [0.99796116, 0.06382433],\n",
       "       [0.99794376, 0.06409609],\n",
       "       [0.9978623 , 0.06535129],\n",
       "       [0.9978515 , 0.06551746],\n",
       "       [0.9979119 , 0.06458977],\n",
       "       [0.99799836, 0.06324073],\n",
       "       [0.9979172 , 0.06450863],\n",
       "       [0.99807984, 0.06194062],\n",
       "       [0.9977148 , 0.06756528],\n",
       "       [0.9976559 , 0.06843004],\n",
       "       [0.99780315, 0.06624894],\n",
       "       [0.99765235, 0.06848173],\n",
       "       [0.99774593, 0.06710504],\n",
       "       [0.9976205 , 0.06894466],\n",
       "       [0.9977454 , 0.06711297],\n",
       "       [0.9975528 , 0.06991784],\n",
       "       [0.9976413 , 0.06864253],\n",
       "       [0.997559  , 0.06982835],\n",
       "       [0.99752885, 0.07025825],\n",
       "       [0.9976642 , 0.06830881],\n",
       "       [0.99768883, 0.06794866],\n",
       "       [0.99761194, 0.06906894],\n",
       "       [0.9978073 , 0.06618638]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train)\n",
    "X_train_norm = normalizer.transform(X_train)\n",
    "X_train_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6833179 , -1.7197781 ],\n",
       "       [ 0.5732904 , -0.56145555],\n",
       "       [ 0.43155304, -0.4164824 ],\n",
       "       [ 0.78328556, -0.7761012 ],\n",
       "       [ 1.5095031 , -1.5348104 ],\n",
       "       [ 0.65684086, -0.64642626],\n",
       "       [ 0.76463586, -0.75711364],\n",
       "       [ 0.97015506, -0.96923274],\n",
       "       [ 0.8612411 , -0.8564363 ],\n",
       "       [ 0.35135952, -0.3354576 ],\n",
       "       [ 0.28384775, -0.26648712],\n",
       "       [ 0.66168976, -0.6515319 ],\n",
       "       [ 1.2029028 , -1.2114629 ],\n",
       "       [ 0.69488615, -0.68520844],\n",
       "       [ 1.7127844 , -1.7510858 ],\n",
       "       [-0.57142544,  0.5834798 ],\n",
       "       [-0.93994266,  0.9424059 ],\n",
       "       [-0.01864966,  0.03712154],\n",
       "       [-0.96232224,  0.96386117],\n",
       "       [-0.37672305,  0.3924542 ],\n",
       "       [-1.1615006 ,  1.1560031 ],\n",
       "       [-0.38007998,  0.39574763],\n",
       "       [-1.5852207 ,  1.5599334 ],\n",
       "       [-1.0313259 ,  1.030602  ],\n",
       "       [-1.5464294 ,  1.5227871 ],\n",
       "       [-1.7351639 ,  1.7012235 ],\n",
       "       [-0.88809663,  0.89208895],\n",
       "       [-0.73405045,  0.74260706],\n",
       "       [-1.2152115 ,  1.207588  ],\n",
       "       [ 0.00745986,  0.0111544 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(X_train_norm)\n",
    "X_train_scaled = scaler.transform(X_train_norm)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a seed to have reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1204374f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a classifier equivalent to a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.layer1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = torch.from_numpy(X_train_scaled).float()\n",
    "y_train = torch.from_numpy(y_train).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_scaled.shape[1]\n",
    "model = Model(input_dim)\n",
    "loss_fn = nn.BCELoss()  # binary cross entropy loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model with a batch size of one item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(50):\n",
    "    for x_train_scaled, y_t in zip(X_train_scaled, y_train):\n",
    "        y_train_pred = model(X_train_scaled)\n",
    "        loss = loss_fn(y_train_pred, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-2.0757,  1.2755]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0259], requires_grad=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the probabilities to belong to class 1 for all the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0035],\n",
       "        [0.1324],\n",
       "        [0.1976],\n",
       "        [0.0698],\n",
       "        [0.0063],\n",
       "        [0.1032],\n",
       "        [0.0740],\n",
       "        [0.0383],\n",
       "        [0.0545],\n",
       "        [0.2439],\n",
       "        [0.2884],\n",
       "        [0.1017],\n",
       "        [0.0177],\n",
       "        [0.0919],\n",
       "        [0.0031],\n",
       "        [0.8761],\n",
       "        [0.9600],\n",
       "        [0.5280],\n",
       "        [0.9628],\n",
       "        [0.7873],\n",
       "        [0.9804],\n",
       "        [0.7891],\n",
       "        [0.9951],\n",
       "        [0.9702],\n",
       "        [0.9944],\n",
       "        [0.9970],\n",
       "        [0.9529],\n",
       "        [0.9239],\n",
       "        [0.9835],\n",
       "        [0.5062]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "predicted_probs = model(X_train_scaled)\n",
    "predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(preds):\n",
    "    c = []\n",
    "    for x in range(len(preds)):\n",
    "        if (preds[x] >= 0.5):\n",
    "            c += [1]\n",
    "        else:\n",
    "            c += [0]\n",
    "    return np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = predict_class(predicted_probs)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We computed the accuracy from the training set. This is not a good practice. We should use a dedicated test set instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
